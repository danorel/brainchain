{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7706afd6-6b3a-41b5-a995-3280e42c892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb3ef2-0687-41b8-b9c2-e0593ba25436",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a341c669-7b75-4404-88d6-c0d2e34e4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.output_parsers import PydanticOutputParser, RetryWithErrorOutputParser\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate, \n",
    "    FewShotChatMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SemanticSimilarityExampleSelector,\n",
    "    SystemMessagePromptTemplate\n",
    ")\n",
    "from langchain.vectorstores import Chroma\n",
    "from pydantic import BaseModel, Field, model_validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "767d72d6-f46a-4cab-b6d6-7b2c7a7d5b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bdd107f-fe31-4542-8ae6-5d526413f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=OPENAI_API_KEY, temperature=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f25dd2e-1818-403b-a526-6ee8c7bd765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sentence(BaseModel):\n",
    "    incorrect: str = Field(description=\"sentence with grammatical and punctuation errors\")\n",
    "    clean: str = Field(description=\"sentence without grammatical and punctuation errors\")\n",
    "    \n",
    "    @model_validator(mode='before')\n",
    "    def clean_sentence_should_differ_from_incorrect_sentence(cls, data):\n",
    "        if data['clean'] == data['incorrect']:\n",
    "            warnings.warn(f\"Not fixed the issue in sentence {data['incorrect']}!\")\n",
    "        return data\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=Sentence)\n",
    "\n",
    "retry_parser = RetryWithErrorOutputParser.from_llm(\n",
    "    parser=parser, llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdb1e2af-f952-491d-b1af-c7c52f232341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def read_source(source):\n",
    "    with open(f\"./data/gec-only/{source}.txt\") as f:\n",
    "        return list(filter(None, re.sub('# \\d\\d\\d\\d', '', f.read()).split('\\n')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d533a46b-f18a-4af4-8850-431dedbc9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect, clean = (\n",
    "    read_source(source=\"train.src\"),\n",
    "    read_source(source=\"train.tgt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b81faa27-565d-4f7f-9f33-6012a7a36db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_validation_split(incorrect, clean, ratio = 0.2, subset = 1.0):\n",
    "    size = int(subset * len(incorrect))\n",
    "    indices = set(range(size))\n",
    "    train_indices = set(random.sample(list(indices), k=int((1 - ratio) * size)))\n",
    "    valid_indices = indices.difference(train_indices)\n",
    "    train_incorrect, train_clean = [incorrect[i] for i in train_indices], [clean[i] for i in train_indices]\n",
    "    valid_incorrect, valid_clean = [incorrect[i] for i in valid_indices], [clean[i] for i in valid_indices]\n",
    "    return train_incorrect, train_clean, valid_incorrect, valid_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f27269e3-5bdd-4b9a-99c7-c216f7bf916b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_incorrect, train_clean, valid_incorrect, valid_clean = train_validation_split(incorrect, clean, subset=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c04ebecb-fc48-4592-a838-05b63ef6b159",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [\n",
    "    {\n",
    "        \"input\": incorrect_sentence,\n",
    "        \"output\": clean_sentence\n",
    "    }\n",
    "    for incorrect_sentence, clean_sentence in zip(valid_incorrect, valid_clean)\n",
    "]\n",
    "\n",
    "to_vectorize = [\" \".join(example.values()) for example in examples]\n",
    "\n",
    "example_selector = SemanticSimilarityExampleSelector(\n",
    "    vectorstore=Chroma.from_texts(\n",
    "        to_vectorize, \n",
    "        OpenAIEmbeddings(), \n",
    "        metadatas=examples\n",
    "    ),\n",
    "    k=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c665c8d-ba64-4cc8-98d5-326c4de4cd5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SystemMessagePromptTemplate.from_template(\"You are a helpful assistant that makes grammatical and punctuation error correction in Ukrainian sentences.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "501da0a0-b95f-4962-befb-38b6c2f581ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    input_variables=[\"input\"],\n",
    "    example_selector=example_selector,\n",
    "    example_prompt=ChatPromptTemplate.from_messages(\n",
    "        [(\"human\", \"{input}\"), (\"ai\", \"{output}\")]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "296bd304-0cf8-4ff5-8520-821c2b4b4259",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_prompt = HumanMessagePromptTemplate(\n",
    "    prompt=PromptTemplate(\n",
    "        template=\"{input} \\n {format_instructions}\",\n",
    "        input_variables=[\"input\"],\n",
    "        partial_variables={\"format_instructions\": retry_parser.get_format_instructions()},\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df01a476-8b80-4fa9-99ca-b6e30ca94c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'System: You are a helpful assistant that makes grammatical and punctuation error correction in Ukrainian sentences.\\nHuman: Моє бачення Instagram\\nAI: Моє бачення Instagram\\nHuman: Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням – https://www.instagram.com/yevhenii_kanivets/\\nAI: Якщо цікаво подивитися відразу на результат, то щиро прошу за цим посиланням — https://www.instagram.com/yevhenii_kanivets/\\nHuman: Я не знаю, чи це, правильне речення \\n The output should be formatted as a JSON instance that conforms to the JSON schema below.\\n\\nAs an example, for the schema {\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}\\nthe object {\"foo\": [\"bar\", \"baz\"]} is a well-formatted instance of the schema. The object {\"properties\": {\"foo\": [\"bar\", \"baz\"]}} is not well-formatted.\\n\\nHere is the output schema:\\n```\\n{\"properties\": {\"incorrect\": {\"description\": \"sentence with grammatical and punctuation errors\", \"title\": \"Incorrect\", \"type\": \"string\"}, \"clean\": {\"description\": \"sentence without grammatical and punctuation errors\", \"title\": \"Clean\", \"type\": \"string\"}}, \"required\": [\"incorrect\", \"clean\"]}\\n```'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_prompt,\n",
    "        few_shot_prompt,\n",
    "        human_prompt\n",
    "    ]\n",
    ")\n",
    "prompt.format_prompt(input=\"Я не знаю, чи це, правильне речення\").to_string()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58a7cca6-eb25-4968-a4d0-e431d3959a07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentence(incorrect='Я не знаю, чи це, правильне речення', clean='Я не знаю, чи це правильне речення')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser.parse(llm(prompt.format_prompt(input=\"Я не знаю, чи це, правильне речення\").to_string()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcbf44c-f508-414f-9e59-cefc71c01c45",
   "metadata": {},
   "source": [
    "### Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b1d08d34-7f92-461d-a700-fa1ea0b7cf50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import ExactMatchStringEvaluator\n",
    "\n",
    "evaluator = ExactMatchStringEvaluator(\n",
    "    ignore_case=False,\n",
    "    ignore_numbers=True,\n",
    "    ignore_punctuation=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bdf96f3-b455-499d-8800-f7b1331a7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(valid_incorrect, valid_clean):\n",
    "    correct = 0\n",
    "    for input, reference in zip(valid_incorrect, valid_clean):\n",
    "        prompt_value = prompt.format_prompt(input=input)\n",
    "        completion = llm(prompt_value.to_string())\n",
    "        sentence = retry_parser.parse_with_prompt(completion, prompt_value)\n",
    "        score = evaluator.evaluate_strings(prediction=sentence.clean, reference=reference).get(\"score\")\n",
    "        correct += score\n",
    "    accuracy = correct / len(valid_clean)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df996c6e-dc90-454f-af84-9f3a8952f707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d6/7jdh7rbj2f59v10kcc72rsym0000gn/T/ipykernel_55501/1815535585.py:8: UserWarning: Not fixed the issue in sentence Моє бачення Instagram!\n",
      "  warnings.warn(f\"Not fixed the issue in sentence {data['incorrect']}!\")\n"
     ]
    }
   ],
   "source": [
    "print(f\"Accuracy: {evaluate(valid_incorrect, valid_clean) * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f98219b-06c4-41d0-b813-4293e48459bc",
   "metadata": {},
   "source": [
    "### Deploy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7344b6c9-a52e-4cff-a93b-0a3fef9e76bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_incorrect, test_clean = (\n",
    "    read_source(source=\"valid.src\"),\n",
    "    read_source(source=\"valid.tgt\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9df9ace5-42b0-41df-85a0-a7bafa8c53f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Run llm on test_incorrect and save them in \"valid.tgt\" file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brainchain_3.11.3",
   "language": "python",
   "name": "brainchain_3.11.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
